<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head>
    <meta charset="utf-8">
<title>Transformer è§£è¯» - ç”Ÿå¦‚é€†æ—… ä¸€è‹‡ä»¥èˆª</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">




<meta name="description" content="ğŸ¤£åºŸå®…ï½œç©·ğŸ¤©ï½œæƒ³å˜èƒ–ğŸ’ª">





    <meta name="description" content="Transformer æ˜¯ Google Brain å‘è¡¨åœ¨ NIPS2017 çš„è®ºæ–‡ã€ŠAttention is all you needã€‹ä¸­æå‡ºçš„æ¨¡å‹ï¼Œéšç€æ·±åº¦å­¦ä¹ çš„ç«çƒ­ï¼ŒåŸºäºTransformerçš„é¢„è®­ç»ƒæ¨¡å‹å·²ç»å¸­å· NLP é¢†åŸŸï¼Œè¶³è§Transformerçš„é‡è¦æ€§ã€‚ æœ¬æ–‡å°†æŒ‰ç…§è¿™ç¯‡è®ºæ–‡çš„é¡ºåºå¹¶ç»“åˆä¸€å®šçš„ä»£ç è¿›è¡Œè§£è¯»ï¼Œä½†ä¼šè°ƒæ•´è®ºæ–‡ä¸­æŸäº›éƒ¨åˆ†çš„é¡ºåºã€‚">
<meta name="keywords" content="Deep Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer è§£è¯»">
<meta property="og:url" content="https:&#x2F;&#x2F;fansblog.me&#x2F;2021&#x2F;10&#x2F;11&#x2F;transformer&#x2F;index.html">
<meta property="og:site_name" content="ç”Ÿå¦‚é€†æ—… ä¸€è‹‡ä»¥èˆª">
<meta property="og:description" content="Transformer æ˜¯ Google Brain å‘è¡¨åœ¨ NIPS2017 çš„è®ºæ–‡ã€ŠAttention is all you needã€‹ä¸­æå‡ºçš„æ¨¡å‹ï¼Œéšç€æ·±åº¦å­¦ä¹ çš„ç«çƒ­ï¼ŒåŸºäºTransformerçš„é¢„è®­ç»ƒæ¨¡å‹å·²ç»å¸­å· NLP é¢†åŸŸï¼Œè¶³è§Transformerçš„é‡è¦æ€§ã€‚ æœ¬æ–‡å°†æŒ‰ç…§è¿™ç¯‡è®ºæ–‡çš„é¡ºåºå¹¶ç»“åˆä¸€å®šçš„ä»£ç è¿›è¡Œè§£è¯»ï¼Œä½†ä¼šè°ƒæ•´è®ºæ–‡ä¸­æŸäº›éƒ¨åˆ†çš„é¡ºåºã€‚">
<meta property="og:locale" content="zh-cn">
<meta property="og:image" content="https:&#x2F;&#x2F;fansblog.me&#x2F;2021&#x2F;10&#x2F;11&#x2F;transformer&#x2F;transformer.png">
<meta property="og:image" content="https:&#x2F;&#x2F;fansblog.me&#x2F;2021&#x2F;10&#x2F;11&#x2F;transformer&#x2F;Encoder-Decoder.png">
<meta property="og:image" content="https:&#x2F;&#x2F;fansblog.me&#x2F;2021&#x2F;10&#x2F;11&#x2F;transformer&#x2F;scaledDotProductAttn.png">
<meta property="og:image" content="https:&#x2F;&#x2F;fansblog.me&#x2F;2021&#x2F;10&#x2F;11&#x2F;transformer&#x2F;QKV.png">
<meta property="og:image" content="https:&#x2F;&#x2F;fansblog.me&#x2F;2021&#x2F;10&#x2F;11&#x2F;transformer&#x2F;multiHead.png">
<meta property="og:image" content="https:&#x2F;&#x2F;fansblog.me&#x2F;2021&#x2F;10&#x2F;11&#x2F;transformer&#x2F;multiHeadAttn.png">
<meta property="og:image" content="https:&#x2F;&#x2F;fansblog.me&#x2F;2021&#x2F;10&#x2F;11&#x2F;transformer&#x2F;BN_LN.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;fansblog.me&#x2F;2021&#x2F;10&#x2F;11&#x2F;transformer&#x2F;encoderLayer.png">
<meta property="og:image" content="https:&#x2F;&#x2F;fansblog.me&#x2F;2021&#x2F;10&#x2F;11&#x2F;transformer&#x2F;scaledDotProductAttn.png">
<meta property="og:image" content="https:&#x2F;&#x2F;fansblog.me&#x2F;2021&#x2F;10&#x2F;11&#x2F;transformer&#x2F;maskAttn1.png">
<meta property="og:image" content="https:&#x2F;&#x2F;fansblog.me&#x2F;2021&#x2F;10&#x2F;11&#x2F;transformer&#x2F;maskAttn2.png">
<meta property="og:image" content="https:&#x2F;&#x2F;fansblog.me&#x2F;2021&#x2F;10&#x2F;11&#x2F;transformer&#x2F;transformer2.png">
<meta property="og:image" content="https:&#x2F;&#x2F;fansblog.me&#x2F;2021&#x2F;10&#x2F;11&#x2F;transformer&#x2F;transformer3.png">
<meta property="og:updated_time" content="2021-10-14T10:06:25.167Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;fansblog.me&#x2F;2021&#x2F;10&#x2F;11&#x2F;transformer&#x2F;transformer.png">





<link rel="icon" href="/favicon-32x32.ico">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">

<link rel="stylesheet" href="/css/style.css">

<script defer src="/js/all.js"></script>


    
    
    
    
    
    
    
    
    
    

    


<link rel="alternate" href="/atom.xml" title="ç”Ÿå¦‚é€†æ—… ä¸€è‹‡ä»¥èˆª" type="application/atom+xml">
</head>
<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                    
                    æµå²š
                    
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item "
               href="/archives">Archives</a>
            
            <a class="navbar-item "
               href="/categories">Categories</a>
            
            <a class="navbar-item "
               href="/friendly-link">FriendlyLink</a>
            
            <a class="navbar-item "
               href="/aboutme">About</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="æœç´¢" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            <div class="navbar-item is-hoverable has-dropdown is-hidden-mobile is-hidden-tablet-only toc">
                <a class="navbar-item" title="ç›®å½•">
                    <i class="fa fa-list"></i>
                </a>
                <div class="navbar-dropdown is-right">
                    
                    
                    
                    
                    <a class="navbar-item" href="#0-åˆæ¬¡è§é¢">&nbsp;&nbsp;<b>0. åˆæ¬¡è§é¢</b></a>
                    
                    
                    <hr class="navbar-divider">
                    
                    
                    <a class="navbar-item" href="#1-Model-Architecture">&nbsp;&nbsp;<b>1. Model Architecture</b></a>
                    
                    
                    
                    <a class="navbar-item" href="#1-1-Positional-Encoding">&nbsp;&nbsp;1.1 Positional Encoding</a>
                    
                    
                    
                    <a class="navbar-item" href="#1-2-Scaled-Dot-Product-Attention">&nbsp;&nbsp;1.2 Scaled Dot-Product Attention</a>
                    
                    
                    
                    <a class="navbar-item" href="#1-3-Multi-Head-Attention">&nbsp;&nbsp;1.3 Multi-Head Attention</a>
                    
                    
                    
                    <a class="navbar-item" href="#1-4-Position-wise-Feed-Forward-Networks">&nbsp;&nbsp;1.4 Position-wise Feed-Forward Networks</a>
                    
                    
                    
                    <a class="navbar-item" href="#1-5-Masked-Multi-Head-Attention">&nbsp;&nbsp;1.5 Masked Multi-Head Attention</a>
                    
                    
                    <hr class="navbar-divider">
                    
                    
                    <a class="navbar-item" href="#2-Transformer-Encoder-ä¸€è§ˆ">&nbsp;&nbsp;<b>2. Transformer Encoder ä¸€è§ˆ</b></a>
                    
                    
                    <hr class="navbar-divider">
                    
                    
                    <a class="navbar-item" href="#3-Transformer-Decoder-ä¸€è§ˆ">&nbsp;&nbsp;<b>3. Transformer Decoder ä¸€è§ˆ</b></a>
                    
                    
                    <hr class="navbar-divider">
                    
                    
                    <a class="navbar-item" href="#4-The-END">&nbsp;&nbsp;<b>4. The END</b></a>
                    
                    
                    <hr class="navbar-divider">
                    
                    
                    <a class="navbar-item" href="#Reference">&nbsp;&nbsp;<b>Reference</b></a>
                    
                </div>
            </div>
            
            
            <a class="navbar-item" title="GitHub" href="https://github.com/aestheticisma" target="_blank" rel="noopener">
                
                <i class="fab fa-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

    <section class="section">
    <div class="container">
    <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            Transformer è§£è¯»
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2021-10-11T14:18:19.000Z" itemprop="datePublished">10æœˆ 11 2021</time>
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Deep-Learning/">Deep Learning</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            32 åˆ†é’Ÿ è¯»å®Œ (çº¦ 4819 å­—)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <p style="text-indent:2em">
Transformer æ˜¯ Google Brain å‘è¡¨åœ¨ NIPS2017 çš„è®ºæ–‡ã€ŠAttention is all you needã€‹ä¸­æå‡ºçš„æ¨¡å‹ï¼Œéšç€æ·±åº¦å­¦ä¹ çš„ç«çƒ­ï¼ŒåŸºäºTransformerçš„é¢„è®­ç»ƒæ¨¡å‹å·²ç»å¸­å· NLP é¢†åŸŸï¼Œè¶³è§Transformerçš„é‡è¦æ€§ã€‚
æœ¬æ–‡å°†æŒ‰ç…§è¿™ç¯‡è®ºæ–‡çš„é¡ºåºå¹¶ç»“åˆä¸€å®šçš„ä»£ç è¿›è¡Œè§£è¯»ï¼Œä½†ä¼šè°ƒæ•´è®ºæ–‡ä¸­æŸäº›éƒ¨åˆ†çš„é¡ºåºã€‚<a id="more"></a>
</p>

<h3 id="0-åˆæ¬¡è§é¢"><a href="#0-åˆæ¬¡è§é¢" class="headerlink" title="0. åˆæ¬¡è§é¢"></a>0. åˆæ¬¡è§é¢</h3><p><img src="/2021/10/11/transformer/transformer.png" alt="transformer.png"></p>
<p style="text-indent:2em">
æˆ‘ä»¬å¯ä»¥ä»ä¸Šå›¾çœ‹åˆ°Transformerçš„æ€»ä½“ç»“æ„ï¼Œå¯ä»¥å‘ç°ï¼Œä»–ä¸ä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œå¦‚ RNNï¼ˆLSTMã€GRUï¼‰æœ‰ç€æ˜¾è‘—çš„ä¸åŒã€‚å¾ªç¯ç¥ç»ç½‘ç»œå¦‚RNNçš„è®­ç»ƒæ˜¯è¿­ä»£çš„ã€ä¸²è¡Œçš„ï¼Œå¿…é¡»è¦ç­‰åˆ°å‰ä¸€ä¸ª step è®¡ç®—å®Œæˆæ‰ä¼šå»è®¡ç®—ä¸‹ä¸€ä¸ª step ï¼Œä¹Ÿå°±æ˜¯åä¸€ä¸ªå•å…ƒçš„è¿ç®—ä¾èµ–äºå‰ä¸€ä¸ªå•å…ƒçš„è¾“å‡ºï¼Œåœ¨è¿™é‡Œä¸å¾—ä¸å†æ¬¡æŒ‡å‡º RNN çš„ä¸¤ä¸ªç¼ºé™·ï¼š

<ul>
<li>æ—¶é—´ç‰‡çš„è®¡ç®—ä¾èµ–é—®é¢˜ï¼Œæ— æ³•å¹¶è¡Œè®¡ç®—</li>
<li>é¡ºåºè®¡ç®—çš„è¿‡ç¨‹ä¸­ä¿¡æ¯ä¼šä¸¢å¤±ï¼Œå°½ç®¡ LSTM ç­‰é—¨æœºåˆ¶åœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£äº†é•¿æœŸä¾èµ–çš„é—®é¢˜ï¼Œä½†åœ¨å¯¹äºç‰¹åˆ«é•¿æœŸçš„ä¾èµ–ç°è±¡ä¸Šï¼ŒLSTM ä¾æ—§æ— èƒ½ä¸ºåŠ›ã€‚</li></ul></p>



<p style="text-indent:2em">
è€Œ Transformer åˆ™å®Œå…¨æ‘’å¼ƒäº† RNNï¼Œè½¬è€Œé‡‡ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf Attention Mechanismï¼‰æ¥ç»˜åˆ¶è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„å…¨å±€ä¾èµ–å…³ç³»ã€‚
</p>

<p style="text-indent:2em">
æ€»ä½“ä¸Šï¼ŒTransformerä¸»è¦åˆ†ä¸º Encoder å’Œ Decoder ä¸¤ä¸ªéƒ¨åˆ†ï¼Œå‰è€…è´Ÿè´£æŠŠè¾“å…¥çš„æ–‡æœ¬åºåˆ—è½¬æ¢æˆéšè—å±‚è¡¨ç¤ºï¼Œä¹Ÿå°±æ˜¯ç¼–ç æˆå…·æœ‰ä¸Šä¸‹æ–‡è¡¨ç¤ºçš„ä¸­é—´å‘é‡ï¼Œä¹‹åé€šè¿‡è§£ç å™¨ï¼ˆDecoderï¼‰å†æŠŠéšè—å±‚è¡¨ç¤ºè§£ç æˆæ–‡æœ¬åºåˆ—ã€‚
</p>

<p><img src="/2021/10/11/transformer/Encoder-Decoder.png" alt="Encoder-Decoder"></p>
<p style="text-indent:2em">
è€Œ Encoder å’Œ Decoder å‡æ˜¯ç”±ç¬¬ä¸€å¼ å›¾ä¸­çš„ Encoder Layer å’Œ Decoder Layer åˆ†åˆ«å †å è€Œæˆï¼ŒåŸè®ºæ–‡ä¸­ä½¿ç”¨ Stacks æ¥å½¢å®¹ã€‚Encoder Layer å’Œ Decoder Layer åˆé•¿å¾—ååˆ†ç›¸ä¼¼ï¼Œä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼Œæœ¬æ–‡ä¹ŸæŒ‰ç…§å¦‚ä¸‹é¡ºåºå±•å¼€ã€‚

<ul>
<li>Model Architecture<ul>
<li>Positional Encoding</li>
<li>Scaled Dot-Product Attention</li>
<li>Multi-Head Attention</li>
<li>Position-wise Feed-Forward Networks</li>
<li>Masked Multi-Head Attention</li>
</ul>
</li>
<li>Transformer Encoder ä¸€è§ˆ</li>
<li>Transformer Decoder ä¸€è§ˆ</li>
<li>The END</li></ul></p>



<h3 id="1-Model-Architecture"><a href="#1-Model-Architecture" class="headerlink" title="1. Model Architecture"></a>1. Model Architecture</h3><h4 id="1-1-Positional-Encoding"><a href="#1-1-Positional-Encoding" class="headerlink" title="1.1 Positional Encoding"></a>1.1 Positional Encoding</h4><p style="text-indent:2em">
å‰é¢æåˆ°ï¼ŒTransformerä¸­æ²¡æœ‰é€’å½’å’Œå·ç§¯æ“ä½œï¼Œå› æ­¤æ¨¡å‹ç¼ºå°‘äº†åºåˆ—çš„é¡ºåºä¿¡æ¯ï¼Œä¸ºäº†è®©æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨åºåˆ—çš„é¡ºåºï¼Œå¿…é¡»åŠ å…¥ä¸€äº›åºåˆ—ä¸­çš„æœ‰å…³äºç›¸å¯¹æˆ–ç»å¯¹ä½ç½®çš„ä¿¡æ¯ã€‚è®ºæ–‡ä¸­æœ€ç»ˆä½¿ç”¨äº† "Positional Encoding"ï¼Œå¹¶å°†å…¶åŠ å…¥åˆ° "the bottom of the encoder and decoder stacks"ï¼Œå³åœ¨ Word Embedding å¤„æ·»åŠ äº†ä¸€ä¸ªä½ç½®åµŒå…¥ï¼Œå› æ­¤è¿™ä¸ªä½ç½®åµŒå…¥çš„ç»´åº¦æ˜¯å’Œè¯ï¼ˆå­—ï¼‰å‘é‡çš„ç»´åº¦ç›¸åŒçš„ã€‚
</p>

<p style="text-indent:2em">
è®ºæ–‡ä¸­ä½¿ç”¨äº† sin å’Œ cos å‡½æ•°çš„çº¿å½¢å˜æ¢æ¥ä½œä¸ºåºåˆ—é¡ºåºçš„ä½ç½®ä¿¡æ¯ï¼š
$$ PE(pos, 2i) = sin(pos/1000^{2i/d_{model}}) $$
$$ PE(pos, 2i+1) = cos(pos/1000^{2i/d_{model}}) $$
</p>

<p>å…¶ä¸­ï¼Œpos æŒ‡å½“å‰å­—ä½äºåºåˆ—çš„ä¸­çš„ä½ç½®ï¼Œå–å€¼èŒƒå›´ä¸º [0, max_sequence_length-1]ï¼›è€Œ i æŒ‡çš„æ˜¯ç»´åº¦ï¼Œå‰é¢æåˆ°ï¼Œä½ç½®åµŒå…¥çš„ç»´åº¦æ˜¯å’Œå­—å‘é‡çš„ç»´åº¦ç›¸åŒçš„ï¼Œå¯¹äºå¶æ•°ç»´åº¦ï¼Œé‡‡ç”¨ sinï¼Œè€Œå¯¹äºå¥‡æ•°ç»´åº¦ï¼Œé‡‡ç”¨ cos å‡½æ•°ã€‚$d_{model}$å³ä½ç½®åµŒå…¥ä¸å­—å‘é‡ç»´åº¦æ•°ã€‚</p>
<p style="text-indent:2em">
ä½ç½®åµŒå…¥åœ¨ $d_{model}$ ä¸Šéšç€ç»´åº¦åºå·çš„å¢å¤§ï¼Œå‘¨æœŸå˜åŒ–è¶Šæ¥è¶Šæ…¢ï¼Œä»æœ€åˆçš„ $2\pi$ å˜åŒ–è‡³ $10000*2\pi$ï¼Œè€Œæ¯ä¸€ä¸ªå­—æˆ–å•è¯éƒ½ä¼šåœ¨æ•´ä¸ª $d_{model}$ ç»´åº¦ä¸Šè·å¾—ä¸åŒå‘¨æœŸçš„ sin ä¸ cos å‡½æ•°çš„å–å€¼ç»„åˆï¼Œæœ€ç»ˆä»¥æ­¤ä½œä¸ºåºåˆ—çš„ä½ç½®ä¿¡æ¯åŠ å…¥è‡³æ¨¡å‹å½“ä¸­ã€‚
ä½ç½®å‘é‡åœ¨è¿™é‡Œæœ‰ä¸¤ä¸ªä½œç”¨ï¼š

<ul>
<li>å†³å®šå½“å‰è¯çš„ä½ç½®</li>
<li>è®¡ç®—åœ¨ä¸€ä¸ªå¥å­ä¸­ä¸åŒçš„è¯ä¹‹é—´çš„è·ç¦»</li></ul></p>



<p>æˆ‘ä»¬å®é™…ç”»ä¸€ä¸‹ä½ç½®åµŒå…¥çš„å›¾åƒï¼Œçºµå‘è§‚å¯Ÿä¸‹å›¾ï¼Œå¯ä»¥å‘ç°éšç€ $d_{model}$ çš„åºå·çš„å¢å¤§ï¼Œä½ç½®åµŒå…¥çš„å˜åŒ–è¶Šæ¥è¶Šå¹³ç¼“ã€‚</p>
<iframe src="../../../../templates/PositionalEncoding.html" width="100%" height="600"></iframe>

<p><br><br></p>
<h4 id="1-2-Scaled-Dot-Product-Attention"><a href="#1-2-Scaled-Dot-Product-Attention" class="headerlink" title="1.2 Scaled Dot-Product Attention"></a>1.2 Scaled Dot-Product Attention</h4><blockquote>
<p>An attention function can be described as mapping a query and a set of key-value pairs to an output,<br>where the query, keys, values, and output are all vectors.</p>
</blockquote>
<p style="text-indent:2em">
ä¸Šè¿°è¿™å¥è¯å‡ºè‡ªåŸè®ºæ–‡ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹äº Self-Attentionï¼Œå®šä¹‰äº†ä¸€ä¸ªqueryã€key å’Œ valuesï¼Œè€Œå‰©ä¸‹çš„å°±æ˜¯ä»–ä»¬ä¸‰ä¸ªä¹‹é—´çš„äº’åŠ¨äº†ã€‚è¿™é‡Œæˆ‘ä»¬åˆ†åˆ«å°†å…¶è¡¨ç¤ºä¸ºQã€Kã€Vã€‚ç›´æ¥ä¸ŠåŸæ–‡å›¾ï¼
</p>

<p><img src="/2021/10/11/transformer/scaledDotProductAttn.png" alt="transformer.png"></p>
<p style="text-indent:2em">
é€šè¿‡ä¸Šå›¾å¯ä»¥çœ‹åˆ°ï¼Œè¿™ä¸ª Scaled Dot-Product Attention ä¸»è¦è¿›è¡Œä»¥ä¸‹å‡ ä¸ªæ“ä½œ

<ul>
<li>a. å°† Q å’Œ K è¿›è¡Œäº†çŸ©é˜µä¹˜æ³•ï¼ˆæ³¨æ„ K éœ€è¦è¢«è½¬ç½®ï¼‰ã€‚</li>
<li>b. å¯¹ a ä¸­ç»“æœè¿›è¡Œäº†Scaledæ“ä½œï¼Œå…¶å®ä¹Ÿå°±æ˜¯é™¤ä»¥äº† $\sqrt{d_{k}}$ ã€‚</li>
<li>c. å°† b çš„ç»“æœæ‰§è¡Œäº†maskæ“ä½œï¼Œè¿™ä¸€æ­¥æ˜¯ä¸ºäº†é˜²æ­¢ä¸‹ä¸€æ­¥çš„ softmax æ¦‚ç‡åŒ–äº† Padding ç­‰æ— ç”¨ä½ç½®ã€‚</li>
<li>d. æœ€åå°† c çš„ç»“æœä¹˜ä»¥çŸ©é˜µ V å³å¾—åˆ°æœ€åç»“æœã€‚</li>
</ul>
</p><p>æ€»ç»“ä¸‹æ¥ï¼Œå°±æ˜¯ä¸€ä¸ªå…¬å¼ï¼š$$ Attention(Q, K, V) = softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V $$<br>è€Œè‡³äºä¸ºä»€ä¹ˆéœ€è¦ Scaledï¼Œä¹Ÿå°±æ˜¯ä¸ºä»€ä¹ˆè¦æ‰§è¡Œ b æ“ä½œï¼ŒåŸæ–‡æ˜¯è¿™ä¹ˆè§£é‡Šçš„ï¼š We suspect that for large values of<br>$d_{k}$, the dot products grow large in magnitude, pushing the softmax function into regions where it has<br>extremely small gradients. ä¹Ÿå°±æ˜¯è¯´ï¼Œä½œè€…è®¤ä¸ºå½“ç»´åº¦å¾ˆå¤§æ—¶ï¼Œç‚¹ç§¯çš„ç»“æœä¼šå¾ˆå¤§ï¼Œä¼šå¯¼è‡´ Softmax çš„æ¢¯åº¦å¾ˆå°ï¼Œä¸ºäº†å‡è½»è¿™ä¸ªå½±å“ï¼Œå¯¹ç‚¹ç§¯è¿›è¡Œç¼©æ”¾ã€‚å½“ç„¶ï¼Œå‡è®¾ Q å’Œ K çš„å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1ã€‚å®ƒä»¬çš„çŸ©é˜µä¹˜ç§¯å°†æœ‰å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º $d_{k}$ï¼Œå› æ­¤ä½¿ç”¨$d_{k}$çš„å¹³æ–¹æ ¹ç”¨äºç¼©æ”¾ã€‚</p>
<p></p>

<p>å…³äºä¸ºä»€ä¹ˆè¦ Scaled çš„æ•°å­¦å±‚é¢çš„ç†è§£è¯·çœ‹ <a href="https://www.zhihu.com/question/339723385/answer/782509914" target="_blank" rel="noopener">è¿™é‡Œ</a></p>
<hr>
<p>ä¸å¤šè¯´ï¼Œæ”¾ä»£ç ï¼</p>
<p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">å‡è®¾ d_k å·²å®šä¹‰</span></span><br><span class="line"><span class="hljs-string">å‡å®š d_q == d_k == d_v</span></span><br><span class="line"><span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ScaledDotProductAttention</span><span class="hljs-params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span></span><br><span class="line">        super(ScaledDotProductAttention, self).__init__()</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, Q, K, V, attn_mask)</span>:</span></span><br><span class="line">        <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">        Q: [batch_size, n_heads, len_q, d_k]</span></span><br><span class="line"><span class="hljs-string">        K: [batch_size, n_heads, len_k, d_k]</span></span><br><span class="line"><span class="hljs-string">        V: [batch_size, n_heads, len_v(=len_k), d_v]</span></span><br><span class="line"><span class="hljs-string">        attn_mask: [batch_size, n_heads, seq_len, seq_len]</span></span><br><span class="line"><span class="hljs-string">        '''</span></span><br><span class="line">        scores = torch.matmul(Q, K.transpose(<span class="hljs-number">-1</span>, <span class="hljs-number">-2</span>)) / np.sqrt(d_k)</span><br><span class="line">        scores.masked_fill_(attn_mask, <span class="hljs-number">-1e9</span>)</span><br><span class="line">        </span><br><span class="line">        attn = nn.Softmax(dim = <span class="hljs-number">-1</span>)(scores)</span><br><span class="line">        context = torch.matmul(attn, V) <span class="hljs-comment"># [batch_size, n_heads, len_q, d_v]</span></span><br><span class="line">        <span class="hljs-keyword">return</span> context, attn</span><br></pre></td></tr></table></figure></p>
<p style="text-indent:2em">
åœ¨ä¸Šè¿°ä»£ç çš„ç¬¬17è¡Œï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°æ‰§è¡Œäº†ä¸€ä¸ª Mask æ“ä½œï¼Œå°† scores çŸ©é˜µçš„å¯¹åº”ä½ç½®å˜æˆäº† -1e9ï¼Œè¿™é‡Œè§£é‡Šä¸€ä¸‹ï¼šå› ä¸ºæˆ‘ä»¬é€šå¸¸è®­ç»ƒæ˜¯ä¸€ä¸ªbatché€å…¥æ¨¡å‹çš„ï¼Œå› æ­¤éœ€è¦ä¿è¯è¿™ä¸ªbatchä¸­çš„ä¾‹å¦‚æ¯ä¸€å¥è¯éœ€è¦ç­‰é•¿ï¼Œæ‰€ä»¥éœ€è¦ç»™æ¯ä¸€å¥è¯ Padding åˆ°è®¾å®šçš„æœ€å¤§é•¿åº¦ï¼Œè€Œç”¨äº Padding çš„å ä½ç¬¦æ˜¯æ²¡æœ‰ä»»ä½•æ–‡æœ¬æ„ä¹‰çš„ï¼Œå¦‚æœä¸åŠ ä»¥æ“ä½œå°±å°† scores è¿›è¡Œ Softmaxï¼Œå°±ä¼šè®©æ²¡æœ‰æ„ä¹‰çš„ Padding éƒ¨åˆ†å‚ä¸äº† Softmax è¿ç®—ï¼ŒSoftmaxå‡½æ•°ä¸º $\sigma(z_{i}) = \frac{e^{z_{i}}}{\sum_{j=1}^{K}e^{z_{i}}}$ï¼Œæˆ‘ä»¬å¯ä»¥é€è¿‡å…¬å¼çœ‹åˆ°ï¼Œ$e^{0}$ æ˜¯æœ‰å€¼çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå°±éœ€è¦ç»™åŸå…ˆ Padding çš„éƒ¨åˆ†åŠ ä¸Šä¸€ä¸ªå¾ˆå¤§çš„è´Ÿæ•°åç½®ï¼Œä½¿å¾— Padding ä½ç½®ç»è¿‡ Softmax ä¸º 0ã€‚
</p>



<h4 id="1-3-Multi-Head-Attention"><a href="#1-3-Multi-Head-Attention" class="headerlink" title="1.3 Multi-Head Attention"></a>1.3 Multi-Head Attention</h4><p style="text-indent:2em">
é‚£ Qã€Kã€V åˆæ˜¯ä»€ä¹ˆå‘¢ï¼ŸçŸ©é˜µï¼Ÿæˆ‘çŸ¥é“å®ƒæ˜¯ä¸€ä¸ªä¸ªçš„çŸ©é˜µï¼Œå®ƒä»¬ä¸‰ä¸ªæ˜¯æ€ä¹ˆæ¥çš„å‘¢ï¼Ÿå…¶å®å®ƒä»¬ä¸‰ä¸ªéƒ½æ˜¯é€šè¿‡å°† Input Embeddingï¼ˆå½“ç„¶è¿˜æœ‰å¯èƒ½æ˜¯Output Embeddingï¼Œè¿™é‡Œé»˜è®¤å·²ç»åŠ ä¸Šäº†ä½ç½®åµŒå…¥ï¼Œæ™®éè€Œè¨€ï¼Œå°±æ˜¯è¿›å…¥åˆ°è¿™ä¸€æ¨¡å—çš„è¾“å…¥ï¼‰çº¿å½¢å˜æ¢å¾—åˆ°çš„ä¸€ä¸ªä¸ªçš„çŸ©é˜µã€‚æˆ‘ä»¬åœ¨è¿™ä¸€æ¨¡å—ï¼Œåˆå§‹åŒ–ä¸‰ä¸ª nn.Linear å±‚ï¼Œåˆ†åˆ«ä¸º$W^{Q}, W^{K}, W^{V}$ï¼Œå³å¯å°†è¾“å…¥ x åˆ†åˆ«çº¿æ€§å˜æ¢æˆ Qã€Kã€Vï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
</p>

<p><img src="/2021/10/11/transformer/QKV.png" alt="QKV"></p>
<p style="text-indent:2em">
é‚£ Multi-Head åˆæ˜¯ä»€ä¹ˆæ„æ€å‘¢ï¼Œç¿»è¯‘æˆä¸­æ–‡ï¼Œå¤šå¤´ï¼Ÿå…¶å®å¾ˆç®€å•ï¼Œä¹‹å‰æˆ‘ä»¬å®šä¹‰çš„ä¸€ç»„ Qã€Kã€V å¯ä»¥è®©ä¸€ä¸ªè¯å…³æ³¨åˆ°ä¸ä»–ç›¸å…³çš„è¯ï¼Œæˆ‘ä»¬ç°åœ¨é€šè¿‡å®šä¹‰å¤šç»„ Qã€Kã€Vï¼Œè®©å®ƒä»¬åˆ†åˆ«å»å…³æ³¨ä¸åŒçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¯ä»¥ç†è§£ä¸ºè®©æˆ‘ä»¬çš„æ¨¡å‹é€è¿‡ä¸åŒçš„è§’åº¦å»çœ‹æ•°æ®ã€‚è®¡ç®—çš„æ–¹å¼å¹¶æ²¡æœ‰å˜åŒ–ã€‚
</p>

<p><img src="/2021/10/11/transformer/multiHead.png" alt="multiHead"></p>
<p style="text-indent:2em">
æœ€åå°†æ¯ä¸ª Head å¾—åˆ°çš„ context å‘é‡ Concat åˆ°ä¸€èµ·ï¼Œå³å¾—åˆ°äº†æˆ‘ä»¬æœ€åéœ€è¦çš„ç»“æœã€‚MultiHead Attention æ•´ä½“ç»“æ„å›¾å¦‚ä¸‹æ‰€ç¤ºï¼š
</p>

<p><img src="/2021/10/11/transformer/multiHeadAttn.png" alt="multiHeadAttn"></p>
<p style="text-indent:2em">
åœ¨è¿™ä¸€éƒ¨åˆ†å±•ç¤ºä»£ç ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å†å±•å¼€è¯´ä¸€ä¸‹æ®‹å·®è¿æ¥å’Œ Layer Normalizationã€‚
</p>

<ul>
<li>æ®‹å·®è¿æ¥</li>
</ul>
<p style="text-indent:2em">
æ®‹å·®ç½‘ç»œæ˜¯åœ¨2015å¹´ã€ŠDeep residual learning for image recognitionã€‹ä¸­æå‡ºçš„ã€‚å…¶å…·ä½“æ“ä½œå¾ˆç®€å•ï¼Œåœ¨æˆ‘ä»¬çš„å®ä¾‹ä¸­ï¼Œå°±æ˜¯å°†æ¨¡å—å‰çš„è¾“å…¥åŠ åˆ°ç»è¿‡æ¨¡å—è®¡ç®—åçš„ç»“æœä¸Šï¼Œå†è¾“é€è‡³ä¸‹ä¸€ç¥ç»å•å…ƒä¸­ã€‚ä¹Ÿå°±æ˜¯
$$ NEXT = X_{embedding} + SelfAttention(Q, K, V) $$
</p>

<ul>
<li>Layer Normalization</li>
</ul>
<p>LNæ˜¯å•¥ï¼Ÿè¿˜æœ‰BNï¼Ÿä¸€å¼ å›¾ææ¸…æ¥šï¼</p>
<p><img src="/2021/10/11/transformer/BN_LN.jpg" alt="BN_LN"></p>
<p style="text-indent:2em">
    Layer Normalization çš„ä½œç”¨å°±æ˜¯æŠŠç¥ç»ç½‘ç»œéšè—å±‚å½’ä¸€åŒ–ä¸ºæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œä»¥ä¾¿äºèµ·åˆ°åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼ŒåŠ é€Ÿæ”¶æ•›çš„ä½œç”¨ã€‚å¦‚ä½•å˜æˆæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œç›¸ä¿¡å¤§å®¶åœ¨æ¦‚ç‡è¯¾æˆ–è€…æ•°ç†ç»Ÿè®¡è¯¾ä¸Šæ—©å·²ç»å­¦è¿‡äº†ã€‚
</p>

<hr>
<p>ç»§ç»­ï¼Œä¸Šä»£ç ï¼</p>
<p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MultiHeadAttention</span><span class="hljs-params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span></span><br><span class="line">        super(MultiHeadAttention, self).__init__()</span><br><span class="line">        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias = <span class="hljs-literal">False</span>)</span><br><span class="line">        self.W_K = nn.Linear(d_model, d_k * n_heads, bias = <span class="hljs-literal">False</span>)</span><br><span class="line">        self.W_V = nn.Linear(d_model, d_v * n_heads, bias = <span class="hljs-literal">False</span>)</span><br><span class="line">        self.fc = nn.Linear(n_heads * d_v, d_model, bias = <span class="hljs-literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, input_Q, input_K, input_V, attn_mask)</span>:</span></span><br><span class="line">        <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">        input_Q: [batch_size, len_q, d_model]</span></span><br><span class="line"><span class="hljs-string">        input_K: [batch_size, len_k, d_model]</span></span><br><span class="line"><span class="hljs-string">        input_V: [batch_size, len_v(=len_k), d_model]</span></span><br><span class="line"><span class="hljs-string">        attn_mask: [batch_size, seq_len, seq_len]</span></span><br><span class="line"><span class="hljs-string">        '''</span></span><br><span class="line">        residual, batch_size = input_Q, input_Q.size(<span class="hljs-number">0</span>)</span><br><span class="line">        <span class="hljs-comment"># (B, S, D) -proj-&gt; (B, S, D_new) -split-&gt; (B, S, H, W) -trans-&gt; (B, H, S, W)</span></span><br><span class="line">        Q = self.W_Q(input_Q).view(batch_size, <span class="hljs-number">-1</span>, n_heads, d_k).transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)  <span class="hljs-comment"># Q: [batch_size, n_heads, len_q, d_k]</span></span><br><span class="line">        K = self.W_K(input_K).view(batch_size, <span class="hljs-number">-1</span>, n_heads, d_k).transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)  <span class="hljs-comment"># K: [batch_size, n_heads, len_k, d_k]</span></span><br><span class="line">        V = self.W_V(input_V).view(batch_size, <span class="hljs-number">-1</span>, n_heads, d_v).transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)  <span class="hljs-comment"># V: [batch_size, n_heads, len_v(=len_k), d_v]</span></span><br><span class="line"></span><br><span class="line">        attn_mask = attn_mask.unsqueeze(<span class="hljs-number">1</span>).repeat(<span class="hljs-number">1</span>, n_heads, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>) <span class="hljs-comment"># attn_mask : [batch_size, n_heads, seq_len, seq_len]</span></span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># context: [batch_size, n_heads, len_q, d_v], attn: [batch_size, n_heads, len_q, len_k]</span></span><br><span class="line">        context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)</span><br><span class="line">        context = context.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).reshape(batch_size, <span class="hljs-number">-1</span>, n_heads * d_v) <span class="hljs-comment"># context: [batch_size, len_q, n_heads * d_v]</span></span><br><span class="line">        output = self.fc(context) <span class="hljs-comment"># [batch_size, len_q, d_model]</span></span><br><span class="line">        <span class="hljs-keyword">return</span> nn.LayerNorm(d_model).cuda()(output + residual), attn</span><br></pre></td></tr></table></figure></p>
<h4 id="1-4-Position-wise-Feed-Forward-Networks"><a href="#1-4-Position-wise-Feed-Forward-Networks" class="headerlink" title="1.4 Position-wise Feed-Forward Networks"></a>1.4 Position-wise Feed-Forward Networks</h4><p style="text-indent:2em">
    è¿™ä¸€éƒ¨åˆ†å°±æ˜¾å¾—æ¯”è¾ƒç®€å•äº†ï¼Œæ€»å…±åŒ…å«ä¸¤ä¸ªçº¿å½¢å±‚å¤–åŠ ä¸€ä¸ªReLUæ¿€æ´»å±‚ï¼Œå…·ä½“å…¬å¼å¦‚ä¸‹æ‰€ç¤ºï¼š
    $$ FFN(x) = max(0, xW_{1} + b_{1})W_{2} + b_{2} $$
    é¦–å…ˆä¸Šä¸€é˜¶æ®µçš„è¾“å‡ºåšä¸€ä¸ªçº¿å½¢å˜æ¢ï¼Œå†ç»è¿‡ä¸€ä¸ªReLUæ¿€æ´»ï¼Œæœ€åå†ç»è¿‡ä¸€ä¸ªçº¿å½¢å˜æ¢ã€‚è¿™å°±æ˜¯è¿™ä¸€éƒ¨åˆ†çš„æ‰€æœ‰æ“ä½œï¼Œå…·ä½“ä»£ç å¦‚ä¸‹ï¼ˆåœ¨è¿™é‡Œå°†åç»­çš„æ®‹å·®è¿æ¥ä¸LNä¹Ÿæ”¾åœ¨ä¸€èµ·å®ç°ï¼‰ï¼š
</p>

<p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PoswiseFeedForwardNet</span><span class="hljs-params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span></span><br><span class="line">        super(PoswiseFeedForwardNet, self).__init__()</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(d_model, d_ff, bias=<span class="hljs-literal">False</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(d_ff, d_model, bias=<span class="hljs-literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, inputs)</span>:</span></span><br><span class="line">        <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">        inputs: [batch_size, seq_len, d_model]</span></span><br><span class="line"><span class="hljs-string">        '''</span></span><br><span class="line">        residual = inputs</span><br><span class="line">        output = self.fc(inputs)</span><br><span class="line">        <span class="hljs-keyword">return</span> nn.LayerNorm(d_model).cuda()(output + residual) <span class="hljs-comment"># [batch_size, seq_len, d_model]</span></span><br></pre></td></tr></table></figure></p>
<hr>
<p>è¿™éƒ¨åˆ†æœ€åï¼Œä¸Šä¸€ä¸ª Encoder Layeræ¨¡å—çš„ç»†èŠ‚å›¾ï¼š<br><img src="/2021/10/11/transformer/encoderLayer.png" alt="encoderLayer"></p>
<h4 id="1-5-Masked-Multi-Head-Attention"><a href="#1-5-Masked-Multi-Head-Attention" class="headerlink" title="1.5 Masked Multi-Head Attention"></a>1.5 Masked Multi-Head Attention</h4><p style="text-indent:2em">
æ•´ä¸ª Transformer æ¨¡å‹çš„å­å•å…ƒç»“æ„åªå‰©ä¸‹ Masked Multi-Head Attention è¿™ä¸€ä¸ªéƒ¨åˆ†äº†ï¼Œç»†å¿ƒçš„æœ‹å‹ä»¬å¯ä»¥å‘ç°ï¼Œè¿™ä¸ªå•å…ƒåªæœ‰åœ¨ Decoder Layer ä¸­å­˜åœ¨ï¼Œåœ¨ Encoder Layer ä¸­å¹¶æ²¡æœ‰è¿™ä¸ªç»“æ„å•å…ƒã€‚é‚£æˆ‘ä»¬ä¸éš¾çŒœå‡ºè¿™ä¸ªéƒ¨åˆ†è®¾ç½®çš„ç›®çš„æ˜¯ä»€ä¹ˆã€‚
</p>
<p style="text-indent:2em">
    åœ¨ä¼ ç»Ÿçš„ Seq2Seq ä¸­ï¼ŒDecoder éƒ¨åˆ†ä½¿ç”¨çš„ä¸€èˆ¬æ˜¯ RNNï¼Œæ‰€ä»¥ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ t æ—¶é—´æ­¥çš„è¯ï¼Œæ¨¡å‹æ˜¯æ— æ³•çœ‹åˆ°å¤§äº t æ—¶é—´æ­¥çš„è¯çš„ï¼ŒåŒæ—¶ï¼Œæˆ‘ä»¬æœ¬æ¥ä¹Ÿæ˜¯ä¸èƒ½å°†æ‰€è¦é¢„æµ‹çš„ç»“æœç›´æ¥æš´éœ²ç»™æ¨¡å‹çš„ã€‚å› æ­¤ï¼Œå¦‚æœ Decoder Layer éƒ¨åˆ†ç»§ç»­åƒ Encoder Layer çš„ Self-Attention ä¸€æ ·ï¼Œå°±ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†æ‰€æœ‰çš„æ­£ç¡®ç­”æ¡ˆå‘Šè¯‰äº†æ¨¡å‹ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å¯¹ Decoder Layer çš„è¾“å…¥éƒ¨åˆ†è¿›è¡Œä¸€å®šçš„å¤„ç†ï¼Œä¹Ÿå°±æ˜¯ Mask ã€‚
</p>

<p style="text-indent:2em">
    è¿™æ³¢å¾ˆå…³é”®å•Šï¼Œæˆ‘åˆè¦ç¥­å‡ºè¿™å¹…å›¾äº†ï¼
</p>

<p><img src="/2021/10/11/transformer/scaledDotProductAttn.png" alt="scaledDotProductAttn.png"></p>
<p style="text-indent:2em">
    åœ¨ 1.2 éƒ¨åˆ†ï¼Œæˆ‘ä»¬æåˆ°ï¼Œå›¾ä¸­çš„ Mask éƒ¨åˆ†æ˜¯ä¸ºäº†é˜²æ­¢ Padding éƒ¨åˆ†å½±å“äº† Softmax æ“ä½œï¼Œé‚£å¦‚æœæˆ‘ä»¬åœ¨è¿™éƒ¨åˆ†çš„åŸºç¡€ä¸Šç»§ç»­æ·»åŠ ä¸€ä¸ª Mask ï¼Œä½¿å¾—å‰åºå•è¯æ— æ³•æ•æ‰åˆ°åç»­å•è¯çš„å…³ç³»ï¼Œé‚£æˆ‘ä»¬çš„ Masked Multi-Head Attention ä¹Ÿå°±å®Œæˆäº†ã€‚å¯æ€ä¹ˆåšå‘¢ï¼Ÿ
</p>

<p style="text-indent:2em">
    é¦–å…ˆç”Ÿæˆä¸€ä¸ªä¸‹ä¸‰è§’å…¨0ï¼Œä¸Šä¸‰è§’å…¨ä¸ºè´Ÿæ— ç©·ï¼ˆ-1e9ï¼‰çš„çŸ©é˜µï¼Œç„¶åå°†å®ƒå’Œ Scaled åçš„çŸ©é˜µç›¸åŠ å³å¯ã€‚å¦‚ä¸‹å›¾ä¸­çš„ä¾‹å­æ‰€ç¤ºï¼š
</p>

<p><img src="/2021/10/11/transformer/maskAttn1.png" alt="maskAttn1.png"></p>
<p>è¿™æ ·çš„çŸ©é˜µç»è¿‡ Softmax åï¼Œè´Ÿæ— ç©·å¤„å°±ä¼šå˜ä¸º 0ï¼Œè€Œå‰©ä¸‹çš„éƒ¨åˆ†ï¼Œå¯ä»¥çœ‹åˆ°ï¼Œåªæœ‰åç»­çš„å•è¯æ‰æœ‰ä¸å…¶å‰é¢å•è¯çš„æ³¨æ„åŠ›æƒé‡ã€‚ä¾‹å¦‚: â€œamâ€ è¿™ä¸ªå•è¯åªæœ‰ä¸ â€œstartâ€ å’Œ â€œIâ€ ä»¥åŠå…¶è‡ªèº«çš„æƒé‡å€¼ï¼Œå®ƒä¸å…¶åç»­çš„ â€œfineâ€ çš„æƒé‡å€¼ä¸º 0ã€‚</p>
<p><img src="/2021/10/11/transformer/maskAttn2.png" alt="maskAttn2.png"></p>
<p style="text-indent:2em">
    æ‰€ä»¥æ€»ç»“ä¸€ä¸‹ï¼ŒMasked Multi-Head Attention å°±æ˜¯åœ¨ Multi-Head Attention çš„åŸºç¡€ä¸Šåœ¨ Scaled ä¹‹å‰å¤šåŠ äº†ä¸€ä¸ª Mask æ“ä½œã€‚
</p>

<h3 id="2-Transformer-Encoder-ä¸€è§ˆ"><a href="#2-Transformer-Encoder-ä¸€è§ˆ" class="headerlink" title="2. Transformer Encoder ä¸€è§ˆ"></a>2. Transformer Encoder ä¸€è§ˆ</h3><p style="text-indent:2em">
    ä¸€ä¸ª Encoder ä¼šåŒ…å« n ä¸ª Encoder Layerï¼Œåœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œn = 6ï¼Œè€Œä¸€ä¸ª Encoder Layer åˆç”±ä¸Šè¿°å‡ ä¸ªæ¨¡å—ç»„æˆçš„ã€‚
</p>

<ol>
<li><p>å­—å‘é‡ä¸ä½ç½®ç¼–ç <br>$$ X = Embedding(X) + Positional_Encoding $$</p>
</li>
<li><p>Self-Attention<br>$$ Q = Linear_{q}(X) = XW_{Q} $$<br>$$ K = Linear_{k}(X) = XW_{K} $$<br>$$ V = Linear_{v}(X) = XW_{V} $$<br>$$ X_{attention} = softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V $$</p>
</li>
<li><p>æ®‹å·®è¿æ¥ä¸LN<br>$$ X_{attention} = X + X_{attention} $$<br>$$ X_{attention} = LayerNorm(X_{attention}) $$</p>
</li>
<li><p>Feed-Forward<br>$$ X_{hidden} = Linear(ReLU(Linear(X_{attention}))) = max(0, X_{attention}W_{1} + b_{1})W_{2} + b_{2} $$</p>
</li>
<li><p>æ®‹å·®è¿æ¥ä¸LN<br>$$ X_{hidden} = X_{attention} + X_{hidden} $$<br>$$ X_{hidden} = LayerNorm(X_{hidden}) $$</p>
</li>
</ol>
<hr>
<p>Show The Code!<br><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">EncoderLayer</span><span class="hljs-params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span></span><br><span class="line">        super(EncoderLayer, self).__init__()</span><br><span class="line">        self.enc_self_attn = MultiHeadAttention()</span><br><span class="line">        self.pos_ffn = PoswiseFeedForwardNet()</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, enc_inputs, enc_self_attn_mask)</span>:</span></span><br><span class="line">        <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">        enc_inputs: [batch_size, src_len, d_model]</span></span><br><span class="line"><span class="hljs-string">        enc_self_attn_mask: [batch_size, src_len, src_len]</span></span><br><span class="line"><span class="hljs-string">        '''</span></span><br><span class="line">        <span class="hljs-comment"># enc_outputs: [batch_size, src_len, d_model], attn: [batch_size, n_heads, src_len, src_len]</span></span><br><span class="line">        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) <span class="hljs-comment"># enc_inputs to same Q,K,V</span></span><br><span class="line">        enc_outputs = self.pos_ffn(enc_outputs) <span class="hljs-comment"># enc_outputs: [batch_size, src_len, d_model]</span></span><br><span class="line">        <span class="hljs-keyword">return</span> enc_outputs, attn</span><br><span class="line"></span><br><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Encoder</span><span class="hljs-params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        self.src_emb = nn.Embedding(src_vocab_size, d_model)</span><br><span class="line">        self.pos_emb = PositionalEncoding(d_model)</span><br><span class="line">        self.layers = nn.ModuleList([EncoderLayer() <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(n_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, enc_inputs)</span>:</span></span><br><span class="line">        <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">        enc_inputs: [batch_size, src_len]</span></span><br><span class="line"><span class="hljs-string">        '''</span></span><br><span class="line">        enc_outputs = self.src_emb(enc_inputs) <span class="hljs-comment"># [batch_size, src_len, d_model]</span></span><br><span class="line">        enc_outputs = self.pos_emb(enc_outputs.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)).transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>) <span class="hljs-comment"># [batch_size, src_len, d_model]</span></span><br><span class="line">        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs) <span class="hljs-comment"># [batch_size, src_len, src_len]</span></span><br><span class="line">        enc_self_attns = []</span><br><span class="line">        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.layers:</span><br><span class="line">            <span class="hljs-comment"># enc_outputs: [batch_size, src_len, d_model], enc_self_attn: [batch_size, n_heads, src_len, src_len]</span></span><br><span class="line">            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)</span><br><span class="line">            enc_self_attns.append(enc_self_attn)</span><br><span class="line">        <span class="hljs-keyword">return</span> enc_outputs, enc_self_attns</span><br></pre></td></tr></table></figure></p>
<h3 id="3-Transformer-Decoder-ä¸€è§ˆ"><a href="#3-Transformer-Decoder-ä¸€è§ˆ" class="headerlink" title="3. Transformer Decoder ä¸€è§ˆ"></a>3. Transformer Decoder ä¸€è§ˆ</h3><p>æˆ‘ä»¬å…ˆä¸Šå›¾ï¼Œæ¯•ç«Ÿ Decoder Layer éƒ¨åˆ†è¿˜æ˜¯æ¯” Encoder Layeréƒ¨åˆ†å¤šäº†ä¸€ç‚¹ä¸œè¥¿çš„ï¼<br><img src="/2021/10/11/transformer/transformer2.png" alt="transformer.png"></p>
<p style="text-indent:2em">
    æˆ‘ä»¬æŠŠç”¨çº¢çº¿æ¡†èµ·æ¥çš„éƒ¨åˆ†å«åš Decoder self attentionï¼Œè€Œç”¨è“è™šçº¿æ¡†èµ·æ¥çš„éƒ¨åˆ†åŒºåˆ†ä¸º Encoder-Decoder attentionã€‚åœ¨å‰é¢æˆ‘ä»¬å·²ç»ä»‹ç»è¿‡äº† Decoder self attention éƒ¨åˆ†ï¼Œå®ƒä»…ä»…æ˜¯å¤šäº†ä¸€ä¸ªä¸Šä¸‰è§’çŸ©é˜µç”¨äºmaskæ‰æœªçŸ¥çš„æƒé‡ä¿¡æ¯ï¼›è€Œåè€…ä¸å‰è¿°çš„ Self-Attention å¹¶æ— ä¸åŒï¼Œå”¯ä¸€çš„åŒºåˆ«åœ¨äºå…¶ä¸­äº§ç”Ÿ Q çš„è¾“å…¥æ¥è‡ªäºå…¶å‰é¢çš„ Decoder self attention éƒ¨åˆ†ï¼Œè€Œ äº§ç”Ÿ K å’Œ V çš„è¾“å…¥æ¥è‡ªäºæœ€åä¸€ä¸ª Encoder Layer çš„è¾“å‡ºï¼Œä¹Ÿå°±æ˜¯æ•´ä¸ª Encoder çš„è¾“å‡ºã€‚
</p>

<hr>
<p>Show The Code!<br><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DecoderLayer</span><span class="hljs-params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span></span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        self.dec_self_attn = MultiHeadAttention()</span><br><span class="line">        self.dec_enc_attn = MultiHeadAttention()</span><br><span class="line">        self.pos_ffn = PoswiseFeedForwardNet()</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)</span>:</span></span><br><span class="line">        <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">        dec_inputs: [batch_size, tgt_len, d_model]</span></span><br><span class="line"><span class="hljs-string">        enc_outputs: [batch_size, src_len, d_model]</span></span><br><span class="line"><span class="hljs-string">        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]</span></span><br><span class="line"><span class="hljs-string">        dec_enc_attn_mask: [batch_size, tgt_len, src_len]</span></span><br><span class="line"><span class="hljs-string">        '''</span></span><br><span class="line">        <span class="hljs-comment"># dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]</span></span><br><span class="line">        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)</span><br><span class="line">        <span class="hljs-comment"># dec_outputs: [batch_size, tgt_len, d_model], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]</span></span><br><span class="line">        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)</span><br><span class="line">        dec_outputs = self.pos_ffn(dec_outputs) <span class="hljs-comment"># [batch_size, tgt_len, d_model]</span></span><br><span class="line">        <span class="hljs-keyword">return</span> dec_outputs, dec_self_attn, dec_enc_attn</span><br><span class="line"></span><br><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Decoder</span><span class="hljs-params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)</span><br><span class="line">        self.pos_emb = PositionalEncoding(d_model)</span><br><span class="line">        self.layers = nn.ModuleList([DecoderLayer() <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(n_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, dec_inputs, enc_inputs, enc_outputs)</span>:</span></span><br><span class="line">        <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">        dec_inputs: [batch_size, tgt_len]</span></span><br><span class="line"><span class="hljs-string">        enc_intpus: [batch_size, src_len]</span></span><br><span class="line"><span class="hljs-string">        enc_outputs: [batsh_size, src_len, d_model]</span></span><br><span class="line"><span class="hljs-string">        '''</span></span><br><span class="line">        dec_outputs = self.tgt_emb(dec_inputs) <span class="hljs-comment"># [batch_size, tgt_len, d_model]</span></span><br><span class="line">        dec_outputs = self.pos_emb(dec_outputs.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)).transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>).cuda() <span class="hljs-comment"># [batch_size, tgt_len, d_model]</span></span><br><span class="line">        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).cuda() <span class="hljs-comment"># [batch_size, tgt_len, tgt_len]</span></span><br><span class="line">        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).cuda() <span class="hljs-comment"># [batch_size, tgt_len, tgt_len]</span></span><br><span class="line">        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask), <span class="hljs-number">0</span>).cuda() <span class="hljs-comment"># [batch_size, tgt_len, tgt_len]</span></span><br><span class="line"></span><br><span class="line">        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs) <span class="hljs-comment"># [batc_size, tgt_len, src_len]</span></span><br><span class="line"></span><br><span class="line">        dec_self_attns, dec_enc_attns = [], []</span><br><span class="line">        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.layers:</span><br><span class="line">            <span class="hljs-comment"># dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]</span></span><br><span class="line">            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)</span><br><span class="line">            dec_self_attns.append(dec_self_attn)</span><br><span class="line">            dec_enc_attns.append(dec_enc_attn)</span><br><span class="line">        <span class="hljs-keyword">return</span> dec_outputs, dec_self_attns, dec_enc_attns</span><br></pre></td></tr></table></figure></p>
<h3 id="4-The-END"><a href="#4-The-END" class="headerlink" title="4. The END"></a>4. The END</h3><p><img src="/2021/10/11/transformer/transformer3.png" alt="transformer.png"></p>
<p style="text-indent:2em">
è¯´æ¥æƒ­æ„§ï¼Œæœ€åˆå»äº†è§£ Transformer çš„æ—¶å€™ï¼Œæˆ‘æ€»æ˜¯æœ‰ä¸ªç–‘æƒ‘ï¼Œæ¯ä¸ªå­å•å…ƒç»“æ„æˆ‘éƒ½çŸ¥é“äº†ï¼Œä½†æ˜¯å®ƒä»¬æ˜¯æ€ä¹ˆçº§è”èµ·æ¥çš„å‘¢ï¼Ÿçœ‹å®Œä¸Šå›¾ï¼Œå°±ä¼šå¤§è‡´æ˜ç™½äº†ï¼Œä¸€å®šè¦æ³¨æ„æ˜¯ Encoder çš„æœ€åä¸€ä¸ª Layer çš„è¾“å‡ºä¼šé€å…¥åˆ° Decoder çš„æ¯ä¸€ä¸ª Layer ä¸­ï¼Œä½œä¸ºåœ¨ Encoder-Decoder attention éƒ¨åˆ†ç”¨äºäº§ç”Ÿ Kã€V çŸ©é˜µã€‚
</p>

<p><figure class="highlight python hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Transformer</span><span class="hljs-params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span></span><br><span class="line">        super(Transformer, self).__init__()</span><br><span class="line">        self.encoder = Encoder().cuda()</span><br><span class="line">        self.decoder = Decoder().cuda()</span><br><span class="line">        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=<span class="hljs-literal">False</span>).cuda()</span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, enc_inputs, dec_inputs)</span>:</span></span><br><span class="line">        <span class="hljs-string">'''</span></span><br><span class="line"><span class="hljs-string">        enc_inputs: [batch_size, src_len]</span></span><br><span class="line"><span class="hljs-string">        dec_inputs: [batch_size, tgt_len]</span></span><br><span class="line"><span class="hljs-string">        '''</span></span><br><span class="line">        <span class="hljs-comment"># tensor to store decoder outputs</span></span><br><span class="line">        <span class="hljs-comment"># outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)</span></span><br><span class="line">        </span><br><span class="line">        <span class="hljs-comment"># enc_outputs: [batch_size, src_len, d_model], enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]</span></span><br><span class="line">        enc_outputs, enc_self_attns = self.encoder(enc_inputs)</span><br><span class="line">        <span class="hljs-comment"># dec_outpus: [batch_size, tgt_len, d_model], dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [n_layers, batch_size, tgt_len, src_len]</span></span><br><span class="line">        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)</span><br><span class="line">        dec_logits = self.projection(dec_outputs) <span class="hljs-comment"># dec_logits: [batch_size, tgt_len, tgt_vocab_size]</span></span><br><span class="line">        <span class="hljs-keyword">return</span> dec_logits.view(<span class="hljs-number">-1</span>, dec_logits.size(<span class="hljs-number">-1</span>)), enc_self_attns, dec_self_attns, dec_enc_attns</span><br></pre></td></tr></table></figure></p>
<p>å®Œæ•´ä»£ç ï¼š<a href="https://github.com/aestheticisma/iWantOffer/blob/main/Transformer/Transformer.ipynb" target="_blank" rel="noopener">https://github.com/aestheticisma/iWantOffer/blob/main/Transformer/Transformer.ipynb</a></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="https://wmathor.com/index.php/archives/1438/" target="_blank" rel="noopener">https://wmathor.com/index.php/archives/1438/</a></li>
<li><a href="https://www.cnblogs.com/zingp/p/11696111.html" target="_blank" rel="noopener">https://www.cnblogs.com/zingp/p/11696111.html</a></li>
<li><a href="https://www.zhihu.com/question/347678607" target="_blank" rel="noopener">https://www.zhihu.com/question/347678607</a></li>
<li><a href="https://colab.research.google.com/drive/15yTJSjZpYuIWzL9hSbyThHLer4iaJjBD?usp=sharing#scrollTo=g831xANXh2HY" target="_blank" rel="noopener">https://colab.research.google.com/drive/15yTJSjZpYuIWzL9hSbyThHLer4iaJjBD?usp=sharing#scrollTo=g831xANXh2HY</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/363466672" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/363466672</a></li>
<li><a href="https://www.zhihu.com/question/339723385/answer/782509914" target="_blank" rel="noopener">https://www.zhihu.com/question/339723385/answer/782509914</a></li>
<li><a href="https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3" target="_blank" rel="noopener">https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3</a></li>
</ul>
<style>
    img {
        width: 55%;
        height: 55%;
        margin: auto;
        display: table-cell;
        text-align: center;
    }
</style>

    
    </div>
    
    <div class="columns is-variable is-1 is-multiline is-mobile">
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/tags/Deep-Learning/">#Deep Learning</a></span>
    
    </div>
    
    
    <div class="columns is-mobile is-multiline article-nav">
        <span class="column is-12-mobile is-half-desktop  article-nav-prev">
            
            <a href="/2021/11/24/deberta/">DeBERTa è®ºæ–‡è§£è¯»</a>
            
        </span>
        <span class="column is-12-mobile is-half-desktop  article-nav-next">
            
            <a href="/2021/06/29/cudaToolkitInstallProblem/">å…³äºCUDA Toolkitå®‰è£…å¤±è´¥è§£å†³æªæ–½æ€»ç»“ï¼ˆUbuntu server 20.04.2ï¼‰</a>
            
        </span>
    </div>
    
</article>






    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2019-2022 æµå²š&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a
                        href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            <div class="column is-narrow">
                <div class="columns is-mobile is-multiline is-centered">
                
                    
                <a class="column is-narrow has-text-black" title="GitHub" href="https://github.com/aestheticisma" target="_blank" rel="noopener">
                    
                    GitHub
                    
                </a>
                
                </div>
            </div>
            
            
        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/" target="_blank" rel="noopener">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("zh-CN");
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
</script>

    
    
    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
            cursor: pointer;
        }

        .hljs .clipboard-btn:hover {
          color: #8a8a8a;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
        clipboard.on('success', function(e) {
          e.clearSelection();
        })
      })
    </script>

    
    

    


<script src="/js/script.js"></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="ç«™å†…æœç´¢" />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'æ–‡ç« ',
                PAGES: 'é¡µé¢',
                CATEGORIES: 'åˆ†ç±»',
                TAGS: 'æ ‡ç­¾',
                UNTITLED: '(æ— æ ‡é¢˜)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js"></script>
    
</body>
</html>